"""
Dummy LLM provider for testing and development.

Returns fixed responses without making any network calls.
"""

from app.core.llm_adapter import LLMProvider


class DummyLLMProvider(LLMProvider):
    """Dummy LLM provider that returns fixed responses."""
    
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generate a dummy response based on the prompt type.
        
        Args:
            prompt: The input prompt (not used in dummy implementation)
            **kwargs: Additional parameters (ignored)
            
        Returns:
            Fixed dummy response
        """
        # Return different responses based on prompt content
        prompt_lower = prompt.lower()
        
        if "explain" in prompt_lower:
            return "This is a dummy explanation of the SQL query. In a real implementation, this would be generated by an LLM based on the query structure and database schema."
        
        elif "optimize" in prompt_lower:
            return "Dummy optimization suggestion: Consider adding indexes on frequently queried columns and reviewing join conditions for better performance."
        
        elif "lint" in prompt_lower:
            return "Dummy linting result: The SQL query appears to be syntactically correct. Consider using explicit column names instead of SELECT * for better maintainability."
        
        else:
            return "This is a dummy response from the LLM provider. The actual implementation would generate a contextual response based on the input prompt."
    
    def is_available(self) -> bool:
        """
        Check if the dummy provider is available.
        
        Returns:
            Always True for dummy provider
        """
        return True
